<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Efficient Long Video Tokenization via Coordinated-based Patch Reconstrution">
  <meta name="keywords" content="CoordTok, Video tokenization, Long video tokenization, Video generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Efficient Long Video Tokenization via Coordinated-based Patch Reconstrution</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://huiwon-jang.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://sites.google.com/view/2024rsp">
            RSP
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Efficient Long Video Tokenization via Coordinated-based Patch Reconstrution</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://huiwon-jang.github.io/">Huiwon Jang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sihyun.me/">Sihyun Yu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://alinlab.kaist.ac.kr/shin.html">Jinwoo Shin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://younggyo.me/">Younggyo Seo</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST,</span>
            <span class="author-block"><sup>2</sup>UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="x"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="x"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/huiwon-jang/CoordTok"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <b>TL; DR.</b> We introduce CoordTok, a scalable video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Efficient tokenization of videos remains a challenge in training vision models that can process long videos.
            One promising direction is to develop a tokenizer that can encode long video clips,
            as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization.
            However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once.
            In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos,
            inspired by recent advances in 3D generative models.
            In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled $$(x, y, t)$$ coordinates.
            This allows for training large tokenizer models directly on long videos without requiring excessive training resources.
            Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips.
            For instance, CoordTok can encode a 128-frame video with 128$$\times$$128 resolution into 1280 tokens,
            while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality.
            We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method overview</h2>
        <div class="column has-text-centered">
          <img src="./static/images/method_overview.png" alt>
        </div>
        <div class="content has-text-justified">
          <p>We design our encoder to encode a video $$\mathbf{x}$$ into factorized triplane representations $$\mathbf{z} = \left[\mathbf{z}^{xy}, \mathbf{z}^{yt}, \mathbf{z}^{xt}\right]$$
            which can efficiently represent the video with three 2D latent planes.
            Given the triplane representations $$\mathbf{z}$$,
            our decoder learns a mapping from $$(x, y, t)$$ coordinates to RGB pixels within the corresponding patches.
            In particular, we extract coordinate-based representations of $$N$$ sampled coordinates by querying the coordinates from triplane representations via bilinear interpolation.
            Then the decoder aggregates and fuses information from different coordinates with self-attention layers and project outputs into corresponding patches.
          </p>
          <p>This design enables us to train tokenizers on long videos in a compute-efficient manner by avoiding reconstruction of entire frames at once.
          </p>
        </div>
      </div>
    </div>
    <!--/ Method overview. -->
  </div>
</section>

<style>
  .row {
    display: flex;
    align-items: center; /* 수직 가운데 정렬 */
    margin-bottom: 1px; /* 위아래 간격 줄이기 */
    flex-wrap: wrap; /* 칸이 작아지면 텍스트가 아래로 이동 */
  }
  .label {
    width: 12%; /* 텍스트 너비 */
    text-align: center; /* 텍스트 오른쪽 정렬 */
    font-size: 13px; /* 글자 크기 조정 */
    word-wrap: break-word; /* 긴 텍스트를 적절히 끊음 */
  }
  .video-container {
    width: 88%; /* 비디오 컨테이너 너비 */
    text-align: center; /* 비디오 가운데 정렬 */
  }
  video {
    width: 100%; /* 비디오 크기 조정 */
  }
</style>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Long video reconstruction. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Long video reconstruction</h2>

        <!-- Comparison -->
        <h3 class="title is-4">Comparison with existing video tokenizers</h3>
        <div class="row">
          <div class="label"><b>GT</b></div>
          <div class="video-container">
            <video id="gt" autoplay controls muted loop playsinline>
              <source src="./static/videos/recon_gt.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="row">
          <div class="label"><a href="https://arxiv.org/abs/2302.07685"><b>PVDM</b></a></div>
          <div class="video-container">
            <video id="pvdm" autoplay controls muted loop playsinline>
              <source src="./static/videos/recon_pvdm.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="row">
          <div class="label"><a href="https://arxiv.org/abs/2410.21264"><b>LARP</b></a></div>
          <div class="video-container">
            <video id="larp" autoplay controls muted loop playsinline>
              <source src="./static/videos/recon_larp.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="row">
          <div class="label"><b>CoordTok (Ours)</b></div>
          <div class="video-container">
            <video id="CoordTok (Ours)" autoplay controls muted loop playsinline>
              <source src="./static/videos/recon_ours.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Comparison. -->

        <!-- Qualitative results. -->
        <h3 class="title is-4">More qualitative results</h3>
        <div class="content has-text-justified">
          <p>
            xxxx
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Qualitative results. -->
      </div>
    </div>
    <!--/ Long video reconstruction. -->

    <!-- Long video generation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Long video generation</h2>
        <!-- Qualitative results. -->
        <h3 class="title is-4">Unconditional 128-frame video generation</h3>
        <div class="content has-text-justified">
          <p>
            xxxx
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Qualitative results. -->
      </div>
    </div>
    <!--/ Long video generation. -->
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
